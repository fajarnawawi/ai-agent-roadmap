{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Project 7: The JSON Specialist\n",
    "\n",
    "**Objective:** Fine-tune a small language model to consistently output valid JSON.\n",
    "\n",
    "## üìñ Why Fine-Tune?\n",
    "\n",
    "Base models sometimes:\n",
    "- Output invalid JSON syntax\n",
    "- Add markdown formatting (```json)\n",
    "- Include explanatory text\n",
    "- Miss required fields\n",
    "\n",
    "Fine-tuning teaches consistent formatting!\n",
    "\n",
    "## üéØ QLoRA Benefits\n",
    "\n",
    "- **Memory Efficient:** 4-bit quantization reduces VRAM needs\n",
    "- **Fast:** Only train small adapter layers\n",
    "- **Effective:** Performance close to full fine-tuning\n",
    "- **Merge-able:** Can merge adapters back into base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run in Colab or local GPU environment)\n",
    "# !pip install transformers datasets peft bitsandbytes accelerate trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Create Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create diverse JSON training examples\n",
    "training_data = [\n",
    "    {\n",
    "        \"instruction\": \"Get the weather for San Francisco\",\n",
    "        \"output\": {\"tool\": \"get_weather\", \"args\": {\"city\": \"San Francisco\"}}\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Calculate 125 multiplied by 8\",\n",
    "        \"output\": {\"tool\": \"calculator\", \"args\": {\"expression\": \"125 * 8\"}}\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Search for information about transformers\",\n",
    "        \"output\": {\"tool\": \"web_search\", \"args\": {\"query\": \"transformers\"}}\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Create a user profile for John Doe, age 30\",\n",
    "        \"output\": {\"action\": \"create_profile\", \"data\": {\"name\": \"John Doe\", \"age\": 30}}\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"List the top 5 AI frameworks\",\n",
    "        \"output\": {\n",
    "            \"type\": \"list\",\n",
    "            \"items\": [\"TensorFlow\", \"PyTorch\", \"JAX\", \"Keras\", \"Scikit-learn\"]\n",
    "        }\n",
    "    },\n",
    "    # Add more examples for better fine-tuning\n",
    "    {\n",
    "        \"instruction\": \"Book a flight from NYC to LAX on May 15th\",\n",
    "        \"output\": {\n",
    "            \"tool\": \"book_flight\",\n",
    "            \"args\": {\n",
    "                \"origin\": \"NYC\",\n",
    "                \"destination\": \"LAX\",\n",
    "                \"date\": \"2024-05-15\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Send an email to john@example.com with subject 'Meeting'\",\n",
    "        \"output\": {\n",
    "            \"tool\": \"send_email\",\n",
    "            \"args\": {\n",
    "                \"to\": \"john@example.com\",\n",
    "                \"subject\": \"Meeting\",\n",
    "                \"body\": \"\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "# Format for training\n",
    "def format_instruction(example):\n",
    "    \"\"\"\n",
    "    Format as: Instruction ‚Üí JSON output\n",
    "    \"\"\"\n",
    "    output_json = json.dumps(example['output'], indent=2)\n",
    "    return {\n",
    "        \"text\": f\"\"\"### Instruction:\n",
    "{example['instruction']}\n",
    "\n",
    "### Response:\n",
    "{output_json}\"\"\"\n",
    "    }\n",
    "\n",
    "formatted_data = [format_instruction(ex) for ex in training_data]\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "print(f\"‚úÖ Created dataset with {len(dataset)} examples\\n\")\n",
    "print(\"Example:\")\n",
    "print(dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Load Model with 4-bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"  # Or \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model\n",
    "print(\"üì• Loading model (this may take a few minutes)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully\")\n",
    "print(f\"   Memory footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank\n",
    "    lora_alpha=32,  # Alpha parameter\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Which layers to adapt\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\n‚úÖ LoRA adapters added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./json_specialist\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"üöÄ Starting training...\\n\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Test and Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_json(instruction: str, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Generate JSON from instruction.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract just the response part\n",
    "    response = result.split(\"### Response:\")[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test cases\n",
    "test_instructions = [\n",
    "    \"Get weather for Tokyo\",\n",
    "    \"Calculate 456 divided by 12\",\n",
    "    \"Create a task with title 'Learn LoRA' and priority high\",\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing fine-tuned model:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for instruction in test_instructions:\n",
    "    print(f\"\\nInstruction: {instruction}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    output = generate_json(instruction, model, tokenizer)\n",
    "    print(f\"Output:\\n{output}\")\n",
    "    \n",
    "    # Validate JSON\n",
    "    try:\n",
    "        parsed = json.loads(output)\n",
    "        print(\"‚úÖ Valid JSON\")\n",
    "    except:\n",
    "        print(\"‚ùå Invalid JSON\")\n",
    "    \n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned adapter\n",
    "model.save_pretrained(\"./json_specialist_adapter\")\n",
    "tokenizer.save_pretrained(\"./json_specialist_adapter\")\n",
    "\n",
    "print(\"‚úÖ Model saved to ./json_specialist_adapter\")\n",
    "print(\"\\nTo load later:\")\n",
    "print(\"  model = AutoModelForCausalLM.from_pretrained(...)\")\n",
    "print(\"  model = PeftModel.from_pretrained(model, './json_specialist_adapter')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "### QLoRA Benefits:\n",
    "- **Memory:** 7B model fits in 6GB VRAM (vs 28GB for full)\n",
    "- **Speed:** Faster than full fine-tuning\n",
    "- **Quality:** Minimal performance loss\n",
    "\n",
    "### When to Fine-Tune:\n",
    "- ‚úÖ Specific output format (JSON, code, etc.)\n",
    "- ‚úÖ Domain-specific language\n",
    "- ‚úÖ Consistent behavior patterns\n",
    "- ‚ùå General knowledge (use RAG instead)\n",
    "\n",
    "### Production Tips:\n",
    "- Collect real failure cases for training data\n",
    "- Validate outputs in production\n",
    "- A/B test base vs fine-tuned\n",
    "- Monitor for distribution drift\n",
    "\n",
    "### Next: Deployment!\n",
    "Module 8 wraps this in a production API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
