{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üïµÔ∏è Project 4: The Data Detective\n",
    "\n",
    "**Objective:** Build an agent that queries PDFs and verifies facts on Wikipedia.\n",
    "\n",
    "## üìñ What You'll Learn\n",
    "\n",
    "- RAG (Retrieval Augmented Generation) architecture\n",
    "- Vector databases (ChromaDB)\n",
    "- Document chunking and embedding\n",
    "- Multi-source routing (PDF vs Web)\n",
    "- Citation and source tracking\n",
    "\n",
    "## üéØ Architecture\n",
    "\n",
    "```\n",
    "User Question\n",
    "     |\n",
    "     v\n",
    "  [Router] --> Is this in the PDF or needs web search?\n",
    "     |\n",
    "     +---> [Vector Search] --> PDF chunks\n",
    "     |\n",
    "     +---> [Wikipedia API] --> Web facts\n",
    "     |\n",
    "     v\n",
    "  [LLM + Context] --> Answer with citations\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install chromadb pypdf sentence-transformers wikipedia-api openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "import chromadb\n",
    "from pypdf import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import wikipediaapi\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Create Sample PDF Document\n",
    "\n",
    "We'll create a sample document about AI agents for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample document content (simulating PDF content)\n",
    "SAMPLE_DOCUMENT = \"\"\"\n",
    "AI Agent Systems: Technical Overview\n",
    "\n",
    "Introduction\n",
    "AI agents are autonomous systems that perceive their environment, make decisions, and take actions to achieve specific goals. Modern AI agents combine large language models (LLMs) with tool use, memory systems, and planning capabilities.\n",
    "\n",
    "Core Components\n",
    "\n",
    "1. Profile: The agent's identity and capabilities\n",
    "An agent's profile defines its role, expertise, and available tools. This includes system prompts that shape the agent's behavior and decision-making patterns.\n",
    "\n",
    "2. Memory: Short-term and long-term storage\n",
    "Memory systems enable agents to maintain context across interactions. Short-term memory uses the LLM's context window, while long-term memory typically employs vector databases like ChromaDB or Pinecone for persistent storage.\n",
    "\n",
    "3. Planning: Breaking down complex tasks\n",
    "Planning mechanisms allow agents to decompose large goals into executable steps. Common approaches include Chain-of-Thought prompting, Tree-of-Thought reasoning, and ReAct (Reasoning + Acting) patterns.\n",
    "\n",
    "4. Action: Tool use and execution\n",
    "Agents interact with external systems through tools. These can include APIs, databases, file systems, web browsers, and code interpreters. Tool use follows a pattern: the LLM generates a tool call, the system executes it, and results are fed back to the LLM.\n",
    "\n",
    "RAG Architecture\n",
    "Retrieval Augmented Generation (RAG) enhances LLMs by retrieving relevant information before generating responses. The process involves:\n",
    "1. Chunking documents into smaller segments\n",
    "2. Creating embeddings for each chunk\n",
    "3. Storing embeddings in a vector database\n",
    "4. Retrieving relevant chunks based on query similarity\n",
    "5. Passing retrieved context to the LLM for generation\n",
    "\n",
    "Technical Implementation\n",
    "A production RAG system requires careful consideration of chunk size (typically 200-500 tokens), overlap between chunks (50-100 tokens), embedding models (e.g., sentence-transformers), and retrieval strategies (semantic search, hybrid search, or re-ranking).\n",
    "\n",
    "Multi-Agent Systems\n",
    "Advanced applications use multiple specialized agents that collaborate. For example, a software development system might include a Product Manager agent for requirements, a Coder agent for implementation, and a Reviewer agent for quality assurance. Communication between agents can be synchronous (direct calls) or asynchronous (message queues).\n",
    "\n",
    "Performance Considerations\n",
    "Key metrics for agent systems include latency (time to first token and total response time), cost (API calls and token usage), accuracy (task completion rate), and reliability (error handling and fallback mechanisms). Optimization strategies include caching, batching, and model fine-tuning.\n",
    "\n",
    "Conclusion\n",
    "AI agents represent a paradigm shift from passive LLMs to active autonomous systems. By combining reasoning, memory, planning, and action capabilities, they can tackle complex real-world tasks that require multi-step problem solving and external tool integration.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"üìÑ Sample document loaded ({len(SAMPLE_DOCUMENT)} characters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement Document Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 100) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "    \n",
    "    Args:\n",
    "        text: Full document text\n",
    "        chunk_size: Target size in characters\n",
    "        overlap: Overlap between chunks in characters\n",
    "    \n",
    "    Returns:\n",
    "        List of chunk dictionaries with text and metadata\n",
    "    \"\"\"\n",
    "    # Split into paragraphs first\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    chunk_id = 0\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        if len(current_chunk) + len(para) < chunk_size:\n",
    "            current_chunk += para + \"\\n\\n\"\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append({\n",
    "                    'id': f'chunk_{chunk_id}',\n",
    "                    'text': current_chunk.strip(),\n",
    "                    'metadata': {'chunk_id': chunk_id, 'char_count': len(current_chunk)}\n",
    "                })\n",
    "                chunk_id += 1\n",
    "            \n",
    "            # Start new chunk with overlap from previous\n",
    "            overlap_text = current_chunk[-overlap:] if len(current_chunk) > overlap else current_chunk\n",
    "            current_chunk = overlap_text + para + \"\\n\\n\"\n",
    "    \n",
    "    # Add final chunk\n",
    "    if current_chunk:\n",
    "        chunks.append({\n",
    "            'id': f'chunk_{chunk_id}',\n",
    "            'text': current_chunk.strip(),\n",
    "            'metadata': {'chunk_id': chunk_id, 'char_count': len(current_chunk)}\n",
    "        })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Chunk the document\n",
    "chunks = chunk_text(SAMPLE_DOCUMENT, chunk_size=600, overlap=100)\n",
    "\n",
    "print(f\"‚úÖ Created {len(chunks)} chunks\\n\")\n",
    "for i, chunk in enumerate(chunks[:3], 1):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(f\"  Length: {len(chunk['text'])} chars\")\n",
    "    print(f\"  Preview: {chunk['text'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Create Vector Store with ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Create or get collection\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"ai_agents_doc\",\n",
    "    metadata={\"description\": \"AI Agent technical documentation\"}\n",
    ")\n",
    "\n",
    "# Initialize embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"üì¶ ChromaDB collection created\")\n",
    "print(f\"üìê Embedding model loaded (dimension: 384)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add documents to ChromaDB\n",
    "print(\"üîÑ Adding documents to vector store...\\n\")\n",
    "\n",
    "for chunk in chunks:\n",
    "    # Create embedding\n",
    "    embedding = embedding_model.encode(chunk['text']).tolist()\n",
    "    \n",
    "    # Add to ChromaDB\n",
    "    collection.add(\n",
    "        ids=[chunk['id']],\n",
    "        embeddings=[embedding],\n",
    "        documents=[chunk['text']],\n",
    "        metadatas=[chunk['metadata']]\n",
    "    )\n",
    "    print(f\"‚úì Added {chunk['id']}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Vector store ready with {collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Implement Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_documents(query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Search the vector store for relevant chunks.\n",
    "    \"\"\"\n",
    "    # Create query embedding\n",
    "    query_embedding = embedding_model.encode(query).tolist()\n",
    "    \n",
    "    # Query ChromaDB\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    # Format results\n",
    "    retrieved = []\n",
    "    for i in range(len(results['ids'][0])):\n",
    "        retrieved.append({\n",
    "            'id': results['ids'][0][i],\n",
    "            'text': results['documents'][0][i],\n",
    "            'distance': results['distances'][0][i] if 'distances' in results else None,\n",
    "            'metadata': results['metadatas'][0][i]\n",
    "        })\n",
    "    \n",
    "    return retrieved\n",
    "\n",
    "# Test search\n",
    "test_query = \"What are the core components of an AI agent?\"\n",
    "results = search_documents(test_query)\n",
    "\n",
    "print(f\"üîç Query: '{test_query}'\\n\")\n",
    "print(\"üìä Top Results:\\n\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. [{result['id']}]\")\n",
    "    print(f\"   {result['text'][:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Wikipedia Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Wikipedia API\n",
    "wiki = wikipediaapi.Wikipedia(\n",
    "    user_agent='AIAgentProject/1.0',\n",
    "    language='en'\n",
    ")\n",
    "\n",
    "def search_wikipedia(query: str, sentences: int = 3) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Search Wikipedia and return summary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        page = wiki.page(query)\n",
    "        \n",
    "        if not page.exists():\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': f\"No Wikipedia page found for '{query}'\"\n",
    "            }\n",
    "        \n",
    "        # Get first N sentences\n",
    "        summary = page.summary.split('. ')[:sentences]\n",
    "        summary_text = '. '.join(summary) + '.'\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'title': page.title,\n",
    "            'summary': summary_text,\n",
    "            'url': page.fullurl\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "# Test Wikipedia search\n",
    "test_result = search_wikipedia(\"Artificial Intelligence\")\n",
    "print(\"üåê Wikipedia Test:\\n\")\n",
    "print(f\"Title: {test_result.get('title')}\")\n",
    "print(f\"Summary: {test_result.get('summary', 'N/A')[:200]}...\")\n",
    "print(f\"URL: {test_result.get('url')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Build Router Agent\n",
    "\n",
    "The agent decides: Should I search the PDF or Wikipedia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROUTER_PROMPT = \"\"\"You are a routing agent. Given a question, decide which source to query:\n",
    "\n",
    "Sources:\n",
    "1. DOCUMENT - Use for questions about AI agent architecture, RAG, implementation details\n",
    "2. WIKIPEDIA - Use for general knowledge, historical facts, definitions\n",
    "\n",
    "Respond with ONLY one word: DOCUMENT or WIKIPEDIA\n",
    "\n",
    "Examples:\n",
    "Q: What are the core components of an AI agent?\n",
    "A: DOCUMENT\n",
    "\n",
    "Q: Who invented the Transformer architecture?\n",
    "A: WIKIPEDIA\n",
    "\n",
    "Q: How does RAG work?\n",
    "A: DOCUMENT\n",
    "\n",
    "Q: What is machine learning?\n",
    "A: WIKIPEDIA\n",
    "\"\"\"\n",
    "\n",
    "def route_query(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Determine which source to query.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": ROUTER_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": f\"Question: {question}\"}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip().upper()\n",
    "\n",
    "# Test routing\n",
    "test_questions = [\n",
    "    \"What is the chunk size recommendation for RAG?\",\n",
    "    \"What is natural language processing?\",\n",
    "    \"How do multi-agent systems communicate?\"\n",
    "]\n",
    "\n",
    "print(\"üß≠ Testing Router:\\n\")\n",
    "for q in test_questions:\n",
    "    route = route_query(q)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"‚Üí Route: {route}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Complete RAG Agent with Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_sources(question: str, verbose: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Answer question using appropriate source and provide citations.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"=\"*80)\n",
    "        print(f\"üéØ Question: {question}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Step 1: Route the query\n",
    "    route = route_query(question)\n",
    "    if verbose:\n",
    "        print(f\"üß≠ Routing decision: {route}\\n\")\n",
    "    \n",
    "    # Step 2: Retrieve context\n",
    "    if route == \"DOCUMENT\":\n",
    "        if verbose:\n",
    "            print(\"üìÑ Searching document...\\n\")\n",
    "        \n",
    "        results = search_documents(question, top_k=2)\n",
    "        context = \"\\n\\n\".join([r['text'] for r in results])\n",
    "        sources = [f\"Document chunk {r['id']}\" for r in results]\n",
    "        \n",
    "    else:  # WIKIPEDIA\n",
    "        if verbose:\n",
    "            print(\"üåê Searching Wikipedia...\\n\")\n",
    "        \n",
    "        # Extract topic from question (simple approach)\n",
    "        wiki_result = search_wikipedia(question.split()[-1])  # Last word as topic\n",
    "        \n",
    "        if wiki_result['success']:\n",
    "            context = wiki_result['summary']\n",
    "            sources = [wiki_result['url']]\n",
    "        else:\n",
    "            context = \"No information found.\"\n",
    "            sources = []\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"üìö Context retrieved ({len(context)} chars)\\n\")\n",
    "    \n",
    "    # Step 3: Generate answer with LLM\n",
    "    answer_prompt = f\"\"\"Answer the question based ONLY on the provided context. \n",
    "If the context doesn't contain enough information, say so.\n",
    "Keep your answer concise and factual.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": answer_prompt}],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': answer,\n",
    "        'source_type': route,\n",
    "        'sources': sources,\n",
    "        'context': context\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ RAG Agent ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Test the Complete Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Document query\n",
    "result = answer_with_sources(\"What are the four core components of an AI agent?\")\n",
    "\n",
    "print(\"\\nüìù Answer:\", result['answer'])\n",
    "print(\"\\nüîó Sources:\")\n",
    "for source in result['sources']:\n",
    "    print(f\"  - {source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Wikipedia query\n",
    "result = answer_with_sources(\"What is machine learning?\")\n",
    "\n",
    "print(\"\\nüìù Answer:\", result['answer'])\n",
    "print(\"\\nüîó Sources:\")\n",
    "for source in result['sources']:\n",
    "    print(f\"  - {source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Complex query\n",
    "result = answer_with_sources(\"How does retrieval work in RAG architecture?\")\n",
    "\n",
    "print(\"\\nüìù Answer:\", result['answer'])\n",
    "print(\"\\nüîó Sources:\")\n",
    "for source in result['sources']:\n",
    "    print(f\"  - {source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "### What You've Built:\n",
    "\n",
    "A production-grade RAG system with:\n",
    "1. ‚úÖ Document ingestion and chunking\n",
    "2. ‚úÖ Vector embeddings and search\n",
    "3. ‚úÖ Multi-source routing\n",
    "4. ‚úÖ Context-aware generation\n",
    "5. ‚úÖ Source citations\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "- **Chunking Strategy**: Balance between context and specificity\n",
    "- **Embeddings**: Convert text to searchable vectors\n",
    "- **Vector Databases**: Efficient similarity search at scale\n",
    "- **Routing**: Intelligent source selection\n",
    "- **Grounding**: LLM answers constrained to retrieved context\n",
    "\n",
    "### Production Considerations:\n",
    "\n",
    "- Chunk size affects retrieval quality (experiment!)\n",
    "- Embedding model choice impacts accuracy\n",
    "- Re-ranking can improve top-k results\n",
    "- Hybrid search (semantic + keyword) often best\n",
    "- Cache embeddings for repeated queries\n",
    "\n",
    "### Next Phase:\n",
    "\n",
    "In Phase 3, you'll add **persistent memory** so agents remember conversations across sessions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
