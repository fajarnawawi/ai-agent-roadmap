{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Project 2: The Autocomplete Bot\n",
    "\n",
    "**Objective:** Understand how Large Language Models predict the next token.\n",
    "\n",
    "## üìñ What You'll Learn\n",
    "\n",
    "- How LSTMs generate text character-by-character\n",
    "- What attention mechanisms do in Transformers\n",
    "- How sampling strategies (temperature, top-k) affect creativity\n",
    "- The architecture differences between RNNs, LSTMs, and Transformers\n",
    "\n",
    "## üéØ Learning Goals\n",
    "\n",
    "1. Build and train a character-level LSTM text generator\n",
    "2. Visualize attention weights in a pre-trained Transformer\n",
    "3. Experiment with different text generation strategies\n",
    "4. Understand why Transformers revolutionized NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install torch transformers bertviz numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, BertModel, BertTokenizer\n",
    "from bertviz import head_view\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Build a Character-Level LSTM\n",
    "\n",
    "We'll build a simple LSTM that learns to generate text one character at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1: Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training text - AI/ML definitions\n",
    "text = \"\"\"machine learning is the study of computer algorithms that improve automatically through experience. \n",
    "deep learning is part of machine learning based on artificial neural networks. \n",
    "the transformer is a deep learning model that uses self attention mechanisms. \n",
    "attention is all you need for modern natural language processing. \n",
    "reinforcement learning trains agents to make sequential decisions. \n",
    "supervised learning uses labeled data to train predictive models.\"\"\"\n",
    "\n",
    "text = text.lower()  # Normalize to lowercase\n",
    "\n",
    "# Create character-to-index mappings\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "vocab_size = len(chars)\n",
    "sequence_length = 40  # How many characters to look back\n",
    "\n",
    "print(f\"üìö Text length: {len(text)} characters\")\n",
    "print(f\"üî§ Vocabulary size: {vocab_size} unique characters\")\n",
    "print(f\"üìù Characters: {''.join(chars)}\")\n",
    "print(f\"\\nüéØ Sequence length: {sequence_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training sequences\n",
    "def create_sequences(text, seq_length):\n",
    "    \"\"\"\n",
    "    Create input-output pairs for training.\n",
    "    Input: sequence of characters\n",
    "    Output: next character\n",
    "    \"\"\"\n",
    "    X = []  # Input sequences\n",
    "    y = []  # Target characters (next char)\n",
    "    \n",
    "    for i in range(len(text) - seq_length):\n",
    "        sequence = text[i:i + seq_length]\n",
    "        target = text[i + seq_length]\n",
    "        \n",
    "        # Convert to indices\n",
    "        X.append([char_to_idx[ch] for ch in sequence])\n",
    "        y.append(char_to_idx[target])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = create_sequences(text, sequence_length)\n",
    "\n",
    "print(f\"‚úÖ Created {len(X_train)} training sequences\")\n",
    "print(f\"\\nüìä Example:\")\n",
    "print(f\"   Input:  '{text[:sequence_length]}'\")\n",
    "print(f\"   Target: '{text[sequence_length]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Define the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Character-level LSTM for text generation.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Embedding layer: Converts character indices to dense vectors\n",
    "    2. LSTM layer: Processes sequences with memory\n",
    "    3. Fully connected layer: Outputs probability distribution over characters\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=128, num_layers=2):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer: character index -> dense vector\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM layer(s)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, \n",
    "                           batch_first=True, dropout=0.2)\n",
    "        \n",
    "        # Output layer: hidden state -> character probabilities\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        # x shape: (batch_size, sequence_length)\n",
    "        \n",
    "        # Embed characters\n",
    "        embedded = self.embedding(x)  # (batch, seq, embedding_dim)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        if hidden is None:\n",
    "            lstm_out, hidden = self.lstm(embedded)\n",
    "        else:\n",
    "            lstm_out, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        # Take the last output for prediction\n",
    "        last_output = lstm_out[:, -1, :]  # (batch, hidden_dim)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = self.fc(last_output)  # (batch, vocab_size)\n",
    "        \n",
    "        return logits, hidden\n",
    "\n",
    "# Initialize model\n",
    "model = CharLSTM(vocab_size, embedding_dim=64, hidden_dim=128, num_layers=2)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"üèóÔ∏è  Model Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nüìä Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3: Train the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.LongTensor(X_train).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train).to(device)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "print(\"üöÄ Starting training...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Mini-batch training\n",
    "    for i in range(0, len(X_train_tensor), batch_size):\n",
    "        batch_X = X_train_tensor[i:i+batch_size]\n",
    "        batch_y = y_train_tensor[i:i+batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / (len(X_train_tensor) // batch_size)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4: Generate Text with Different Sampling Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_text, length=200, temperature=1.0, top_k=None):\n",
    "    \"\"\"\n",
    "    Generate text using the trained LSTM.\n",
    "    \n",
    "    Args:\n",
    "        seed_text: Initial text to start generation\n",
    "        length: Number of characters to generate\n",
    "        temperature: Controls randomness (higher = more random)\n",
    "        top_k: If set, only sample from top-k most likely characters\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Ensure seed_text is long enough\n",
    "    if len(seed_text) < sequence_length:\n",
    "        seed_text = seed_text.rjust(sequence_length)\n",
    "    \n",
    "    generated = seed_text\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            # Get last sequence_length characters\n",
    "            context = generated[-sequence_length:]\n",
    "            \n",
    "            # Convert to indices\n",
    "            x = torch.LongTensor([[char_to_idx[ch] for ch in context]]).to(device)\n",
    "            \n",
    "            # Get predictions\n",
    "            logits, _ = model(x)\n",
    "            \n",
    "            # Apply temperature\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "            \n",
    "            # Apply top-k filtering if specified\n",
    "            if top_k is not None:\n",
    "                top_k_indices = np.argpartition(probs, -top_k)[-top_k:]\n",
    "                top_k_probs = probs[top_k_indices]\n",
    "                top_k_probs = top_k_probs / top_k_probs.sum()  # Renormalize\n",
    "                \n",
    "                next_idx = np.random.choice(top_k_indices, p=top_k_probs)\n",
    "            else:\n",
    "                # Sample from full distribution\n",
    "                next_idx = np.random.choice(len(probs), p=probs)\n",
    "            \n",
    "            # Append predicted character\n",
    "            generated += idx_to_char[next_idx]\n",
    "    \n",
    "    return generated\n",
    "\n",
    "# Test with different settings\n",
    "seed = \"machine learning is\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üé≤ Experimenting with Sampling Strategies\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£  Low Temperature (t=0.5) - More Deterministic:\")\n",
    "print(\"-\" * 80)\n",
    "print(generate_text(model, seed, length=150, temperature=0.5))\n",
    "\n",
    "print(f\"\\n\\n2Ô∏è‚É£  High Temperature (t=1.5) - More Creative:\")\n",
    "print(\"-\" * 80)\n",
    "print(generate_text(model, seed, length=150, temperature=1.5))\n",
    "\n",
    "print(f\"\\n\\n3Ô∏è‚É£  Top-K Sampling (k=5) - Balanced:\")\n",
    "print(\"-\" * 80)\n",
    "print(generate_text(model, seed, length=150, temperature=1.0, top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Visualize Transformer Attention\n",
    "\n",
    "Now let's explore how modern Transformers use attention mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Load Pre-trained GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 (small version)\n",
    "print(\"üì• Loading GPT-2...\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2', output_attentions=True)\n",
    "gpt2_model.eval()\n",
    "\n",
    "print(\"‚úÖ GPT-2 loaded successfully!\")\n",
    "print(f\"\\nüìä Model info:\")\n",
    "print(f\"   - Parameters: {sum(p.numel() for p in gpt2_model.parameters()):,}\")\n",
    "print(f\"   - Layers: 12\")\n",
    "print(f\"   - Attention heads: 12\")\n",
    "print(f\"   - Hidden size: 768\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Visualize Attention Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze a sentence\n",
    "sentence = \"The AI agent uses vector search to retrieve relevant documents from memory.\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = gpt2_tokenizer(sentence, return_tensors='pt')\n",
    "tokens = gpt2_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "print(f\"üìù Sentence: {sentence}\")\n",
    "print(f\"\\nüî§ Tokens: {tokens}\")\n",
    "print(f\"   (Total: {len(tokens)} tokens)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model outputs with attention\n",
    "with torch.no_grad():\n",
    "    outputs = gpt2_model(**inputs)\n",
    "    attentions = outputs.attentions  # Tuple of attention matrices for each layer\n",
    "\n",
    "print(f\"‚úÖ Attention tensors extracted\")\n",
    "print(f\"   - Number of layers: {len(attentions)}\")\n",
    "print(f\"   - Attention shape per layer: {attentions[0].shape}\")\n",
    "print(f\"     (batch_size, num_heads, sequence_length, sequence_length)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention from one layer\n",
    "layer_to_visualize = 6  # Middle layer\n",
    "\n",
    "attention_matrix = attentions[layer_to_visualize][0].mean(dim=0).numpy()  # Average across heads\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(attention_matrix, cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.title(f'Attention Patterns - Layer {layer_to_visualize}', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Key Position (Source Token)')\n",
    "plt.ylabel('Query Position (Target Token)')\n",
    "plt.xticks(range(len(tokens)), tokens, rotation=90)\n",
    "plt.yticks(range(len(tokens)), tokens)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° How to read this:\")\n",
    "print(\"   - Each row shows where token i attends to\")\n",
    "print(\"   - Brighter = stronger attention\")\n",
    "print(\"   - Look for patterns: Does 'agent' attend to 'AI'?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: Generate Text with GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_gpt2(prompt, max_length=50, temperature=1.0, top_k=50, num_return=3):\n",
    "    \"\"\"\n",
    "    Generate text using GPT-2 with different sampling strategies.\n",
    "    \"\"\"\n",
    "    input_ids = gpt2_tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    outputs = gpt2_model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        num_return_sequences=num_return,\n",
    "        do_sample=True,\n",
    "        pad_token_id=gpt2_tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    for i, output in enumerate(outputs):\n",
    "        text = gpt2_tokenizer.decode(output, skip_special_tokens=True)\n",
    "        results.append(text)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test generation\n",
    "prompt = \"An AI agent is a system that\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"üéØ Prompt: '{prompt}'\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüå°Ô∏è  Temperature = 0.7 (Balanced)\")\n",
    "print(\"-\"*80)\n",
    "results = generate_with_gpt2(prompt, temperature=0.7, num_return=2)\n",
    "for i, text in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. {text}\")\n",
    "\n",
    "print(\"\\n\\nüî• Temperature = 1.5 (Creative)\")\n",
    "print(\"-\"*80)\n",
    "results = generate_with_gpt2(prompt, temperature=1.5, num_return=2)\n",
    "for i, text in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Comparative Analysis: LSTM vs Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"‚öñÔ∏è  LSTM vs Transformer: Key Differences\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison = {\n",
    "    \"Aspect\": [\n",
    "        \"Architecture\",\n",
    "        \"Parallelization\",\n",
    "        \"Long-range Dependencies\",\n",
    "        \"Training Speed\",\n",
    "        \"Context Understanding\",\n",
    "        \"Parameters (typical)\"\n",
    "    ],\n",
    "    \"LSTM\": [\n",
    "        \"Recurrent (sequential)\",\n",
    "        \"‚ùå Must process sequentially\",\n",
    "        \"‚ö†Ô∏è Limited (vanishing gradient)\",\n",
    "        \"Slower (sequential)\",\n",
    "        \"Local (limited history)\",\n",
    "        \"Thousands to millions\"\n",
    "    ],\n",
    "    \"Transformer\": [\n",
    "        \"Attention-based (parallel)\",\n",
    "        \"‚úÖ Fully parallelizable\",\n",
    "        \"‚úÖ Excellent (self-attention)\",\n",
    "        \"Faster (GPU-friendly)\",\n",
    "        \"Global (full sequence)\",\n",
    "        \"Millions to billions\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(comparison)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(\"\\nüí° Why Transformers Won:\")\n",
    "print(\"   1. Self-attention allows looking at entire sequence at once\")\n",
    "print(\"   2. Parallelization enables training on massive datasets\")\n",
    "print(\"   3. Better at capturing long-range dependencies\")\n",
    "print(\"   4. Scales effectively with more data and compute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Challenge Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1: Implement Top-P (Nucleus) Sampling\n",
    "\n",
    "Instead of top-k, implement top-p sampling which selects from the smallest set of tokens whose cumulative probability exceeds p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_sampling(probs, p=0.9):\n",
    "    \"\"\"\n",
    "    Nucleus sampling: sample from smallest set of tokens with cumulative prob >= p.\n",
    "    \n",
    "    TODO: Implement this function\n",
    "    Hints:\n",
    "    - Sort probabilities in descending order\n",
    "    - Compute cumulative sum\n",
    "    - Find cutoff where cumsum >= p\n",
    "    - Sample only from those tokens\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Analyze Attention Heads\n",
    "\n",
    "Different attention heads learn different patterns. Analyze what different heads focus on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize individual attention heads from GPT-2\n",
    "# - Pick a layer (e.g., layer 6)\n",
    "# - Visualize each of the 12 heads separately\n",
    "# - Do different heads focus on different patterns?\n",
    "#   (e.g., syntactic vs semantic relationships)\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Build a Word-Level LSTM\n",
    "\n",
    "Modify the character-level LSTM to work at the word level instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a word-level LSTM\n",
    "# Steps:\n",
    "# 1. Tokenize text into words\n",
    "# 2. Create word-to-index mappings\n",
    "# 3. Modify the LSTM to work with word embeddings\n",
    "# 4. Train and compare to character-level model\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **LSTMs for Text Generation**:\n",
    "   - Process sequences one step at a time\n",
    "   - Maintain hidden state (memory)\n",
    "   - Good for learning local patterns\n",
    "   - Limited by sequential processing\n",
    "\n",
    "2. **Transformer Architecture**:\n",
    "   - Self-attention: Every token attends to every other token\n",
    "   - Parallel processing: Much faster training\n",
    "   - Positional encoding: Maintains sequence order\n",
    "   - Multi-head attention: Learns multiple relationship types\n",
    "\n",
    "3. **Sampling Strategies**:\n",
    "   - **Temperature**: Controls randomness\n",
    "     - Low (0.1-0.7): Focused, deterministic\n",
    "     - High (1.0-2.0): Creative, diverse\n",
    "   - **Top-K**: Sample from K most likely tokens\n",
    "   - **Top-P (Nucleus)**: Dynamic selection based on cumulative probability\n",
    "\n",
    "4. **Attention Visualization**:\n",
    "   - Shows what the model \"focuses on\"\n",
    "   - Different heads learn different patterns\n",
    "   - Helps understand model behavior\n",
    "\n",
    "### Why This Matters for AI Agents:\n",
    "\n",
    "- **Agents use LLMs**: Understanding how they generate text helps debug behavior\n",
    "- **Sampling control**: Agents need predictable vs creative outputs in different contexts\n",
    "- **Attention = Reasoning**: Visualizing attention helps understand agent decision-making\n",
    "- **Context windows**: Understanding how models process sequences informs prompt engineering\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "In Phase 2, you'll learn to give these LLMs \"hands\" by connecting them to tools and external knowledge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Additional Resources\n",
    "\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [Attention Is All You Need (Original Paper)](https://arxiv.org/abs/1706.03762)\n",
    "- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [Hugging Face Transformers Documentation](https://huggingface.co/docs/transformers/)\n",
    "- [BertViz: Attention Visualization Tool](https://github.com/jessevig/bertviz)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
