{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¢ Project 1: The Vector Search Engine\n",
    "\n",
    "**Objective:** Build a semantic search system from scratch to understand how AI Agents \"retrieve\" knowledge.\n",
    "\n",
    "## ðŸ“– What You'll Learn\n",
    "\n",
    "- How text is converted to numerical vectors (embeddings)\n",
    "- What cosine similarity measures and why it matters\n",
    "- How to implement nearest neighbor search manually\n",
    "- The foundation of RAG (Retrieval Augmented Generation) systems\n",
    "\n",
    "## ðŸŽ¯ Learning Goals\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "1. Vector representations of text\n",
    "2. Mathematical similarity measures\n",
    "3. How semantic search differs from keyword search\n",
    "4. The fundamentals behind vector databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install numpy pandas scikit-learn sentence-transformers matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Load a Text Dataset\n",
    "\n",
    "We'll create a simple knowledge base about AI and Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents - a mini knowledge base about AI/ML\n",
    "documents = [\n",
    "    \"Machine learning is a subset of artificial intelligence that focuses on algorithms that improve through experience.\",\n",
    "    \"Deep learning uses neural networks with multiple layers to learn hierarchical representations of data.\",\n",
    "    \"Natural language processing enables computers to understand, interpret, and generate human language.\",\n",
    "    \"Reinforcement learning involves agents learning to make decisions by interacting with an environment.\",\n",
    "    \"Computer vision allows machines to interpret and understand visual information from the world.\",\n",
    "    \"Supervised learning trains models on labeled data to make predictions on new, unseen data.\",\n",
    "    \"Unsupervised learning finds patterns in data without explicit labels or targets.\",\n",
    "    \"Transfer learning leverages pre-trained models to solve new but related tasks efficiently.\",\n",
    "    \"Gradient descent is an optimization algorithm that minimizes loss functions by iteratively adjusting parameters.\",\n",
    "    \"Transformers are neural network architectures that use self-attention mechanisms for sequence processing.\",\n",
    "]\n",
    "\n",
    "print(f\"ðŸ“š Loaded {len(documents)} documents into our knowledge base\\n\")\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"{i}. {doc[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Convert Text to Vectors\n",
    "\n",
    "We'll use a pre-trained sentence embedding model to convert text into dense vectors.\n",
    "\n",
    "**Key Concept:** Each document becomes a point in high-dimensional space. Similar documents are close together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained embedding model\n",
    "# We're using 'all-MiniLM-L6-v2' - a small, fast model that produces 384-dimensional vectors\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"ðŸ”„ Converting documents to vectors...\\n\")\n",
    "\n",
    "# Encode all documents into vectors\n",
    "document_embeddings = model.encode(documents)\n",
    "\n",
    "print(f\"âœ… Created embeddings with shape: {document_embeddings.shape}\")\n",
    "print(f\"   - Number of documents: {document_embeddings.shape[0]}\")\n",
    "print(f\"   - Embedding dimensions: {document_embeddings.shape[1]}\")\n",
    "print(f\"\\nðŸ“Š First document vector (first 10 dimensions): {document_embeddings[0][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Implement Cosine Similarity from Scratch\n",
    "\n",
    "**Formula:** \n",
    "$$\\text{cosine\\_similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\times \\|B\\|}$$\n",
    "\n",
    "Where:\n",
    "- $A \\cdot B$ is the dot product\n",
    "- $\\|A\\|$ is the magnitude (L2 norm) of vector A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two vectors.\n",
    "    \n",
    "    Args:\n",
    "        vec1: First vector (1D numpy array)\n",
    "        vec2: Second vector (1D numpy array)\n",
    "    \n",
    "    Returns:\n",
    "        Similarity score between -1 and 1 (1 = identical, 0 = orthogonal, -1 = opposite)\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function using NumPy\n",
    "    # Hint: Use np.dot() for dot product and np.linalg.norm() for magnitude\n",
    "    \n",
    "    # Step 1: Calculate dot product\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    \n",
    "    # Step 2: Calculate magnitudes\n",
    "    magnitude_vec1 = np.linalg.norm(vec1)\n",
    "    magnitude_vec2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    # Step 3: Calculate cosine similarity\n",
    "    similarity = dot_product / (magnitude_vec1 * magnitude_vec2)\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "# Test the function\n",
    "test_vec1 = np.array([1, 2, 3])\n",
    "test_vec2 = np.array([2, 4, 6])  # Perfectly aligned (scaled version)\n",
    "test_vec3 = np.array([1, 0, 0])  # Orthogonal to test_vec1\n",
    "\n",
    "print(\"ðŸ§ª Testing cosine similarity function:\")\n",
    "print(f\"   Similarity between [1,2,3] and [2,4,6]: {cosine_similarity(test_vec1, test_vec2):.4f} (expect ~1.0)\")\n",
    "print(f\"   Similarity between [1,2,3] and [1,0,0]: {cosine_similarity(test_vec1, test_vec3):.4f} (expect ~0.27)\")\n",
    "print(f\"\\nâœ… Test passed! Your implementation is correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Build a Semantic Search Function\n",
    "\n",
    "Now let's create a search function that finds the most relevant documents for a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query: str, top_k: int = 3) -> List[Tuple[int, float, str]]:\n",
    "    \"\"\"\n",
    "    Find the most relevant documents for a given query.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query string\n",
    "        top_k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of (doc_index, similarity_score, document_text) tuples\n",
    "    \"\"\"\n",
    "    # Step 1: Convert query to vector\n",
    "    query_embedding = model.encode([query])[0]\n",
    "    \n",
    "    # Step 2: Calculate similarity with all documents\n",
    "    similarities = []\n",
    "    for i, doc_embedding in enumerate(document_embeddings):\n",
    "        sim = cosine_similarity(query_embedding, doc_embedding)\n",
    "        similarities.append((i, sim, documents[i]))\n",
    "    \n",
    "    # Step 3: Sort by similarity (highest first) and return top_k\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Test the search function\n",
    "test_query = \"How do neural networks learn from images?\"\n",
    "print(f\"ðŸ” Query: '{test_query}'\\n\")\n",
    "print(\"ðŸ“Š Top 3 Results:\\n\")\n",
    "\n",
    "results = semantic_search(test_query, top_k=3)\n",
    "for rank, (doc_id, score, text) in enumerate(results, 1):\n",
    "    print(f\"{rank}. [Score: {score:.4f}] {text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Experiment: Compare Different Queries\n",
    "\n",
    "Try different queries to see how semantic search works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different queries\n",
    "queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How do computers understand images?\",\n",
    "    \"Training models without labels\",\n",
    "    \"Language understanding by machines\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ðŸ” Query: '{query}'\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    results = semantic_search(query, top_k=2)\n",
    "    for rank, (doc_id, score, text) in enumerate(results, 1):\n",
    "        print(f\"{rank}. [Score: {score:.4f}]\\n   {text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Visualization: Document Similarity Matrix\n",
    "\n",
    "Let's visualize how similar all documents are to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity matrix\n",
    "n_docs = len(documents)\n",
    "similarity_matrix = np.zeros((n_docs, n_docs))\n",
    "\n",
    "for i in range(n_docs):\n",
    "    for j in range(n_docs):\n",
    "        similarity_matrix[i][j] = cosine_similarity(\n",
    "            document_embeddings[i], \n",
    "            document_embeddings[j]\n",
    "        )\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(similarity_matrix, cmap='YlOrRd', aspect='auto')\n",
    "plt.colorbar(label='Cosine Similarity')\n",
    "plt.title('Document Similarity Matrix', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Document Index')\n",
    "plt.ylabel('Document Index')\n",
    "plt.xticks(range(n_docs))\n",
    "plt.yticks(range(n_docs))\n",
    "\n",
    "# Add similarity values to cells\n",
    "for i in range(n_docs):\n",
    "    for j in range(n_docs):\n",
    "        text = plt.text(j, i, f'{similarity_matrix[i, j]:.2f}',\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Insights:\")\n",
    "print(\"   - Diagonal values are 1.0 (documents are identical to themselves)\")\n",
    "print(\"   - Brighter colors = higher similarity\")\n",
    "print(\"   - Look for clusters of similar documents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Challenge Exercises\n",
    "\n",
    "Test your understanding with these challenges:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1: Implement Euclidean Distance\n",
    "\n",
    "Besides cosine similarity, another common distance metric is Euclidean distance.\n",
    "\n",
    "**Formula:** \n",
    "$$d(A, B) = \\sqrt{\\sum_{i=1}^{n}(A_i - B_i)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Euclidean distance between two vectors.\n",
    "    \n",
    "    TODO: Implement this function\n",
    "    Hint: Use np.sqrt() and np.sum()\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "# vec1 = np.array([1, 2, 3])\n",
    "# vec2 = np.array([4, 5, 6])\n",
    "# print(f\"Euclidean distance: {euclidean_distance(vec1, vec2):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Add Your Own Documents\n",
    "\n",
    "Expand the knowledge base with your own documents and test queries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add 5 new documents about a topic of your choice\n",
    "new_documents = [\n",
    "    # Add your documents here\n",
    "]\n",
    "\n",
    "# Re-encode all documents\n",
    "# all_documents = documents + new_documents\n",
    "# all_embeddings = model.encode(all_documents)\n",
    "\n",
    "# Test your new search!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Build a Keyword Filter\n",
    "\n",
    "Combine semantic search with keyword filtering. Return only results that contain specific keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query: str, required_keywords: List[str], top_k: int = 3):\n",
    "    \"\"\"\n",
    "    Semantic search with keyword filtering.\n",
    "    \n",
    "    TODO: Implement this function\n",
    "    - First do semantic search\n",
    "    - Then filter results to only include documents containing ALL required_keywords\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Key Takeaways\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **Vector Embeddings**: Text can be represented as numerical vectors that capture semantic meaning\n",
    "\n",
    "2. **Cosine Similarity**: Measures the angle between vectors, not their magnitude\n",
    "   - Perfect for semantic similarity (direction matters more than length)\n",
    "   - Range: -1 (opposite) to 1 (identical)\n",
    "\n",
    "3. **Semantic Search**: Finds conceptually related content, not just keyword matches\n",
    "   - Query: \"How do computers see?\" â†’ Finds \"Computer vision allows machines...\"\n",
    "   - Traditional search would miss this!\n",
    "\n",
    "4. **The Foundation of RAG**: This exact pattern is used in production AI agents:\n",
    "   - Vector Database stores embeddings\n",
    "   - Query gets embedded\n",
    "   - Similarity search retrieves context\n",
    "   - LLM generates answer using retrieved docs\n",
    "\n",
    "### Real-World Applications:\n",
    "\n",
    "- **Chatbots**: Retrieve relevant knowledge before answering\n",
    "- **Recommendation Systems**: Find similar products/content\n",
    "- **Document Search**: Semantic search across large corpora\n",
    "- **Question Answering**: RAG systems (Module 4!)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "In Module 2, you'll explore how LLMs themselves work - the models that both generate embeddings AND produce text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Additional Resources\n",
    "\n",
    "- [Sentence Transformers Documentation](https://www.sbert.net/)\n",
    "- [Understanding Word Embeddings](https://jalammar.github.io/illustrated-word2vec/)\n",
    "- [Vector Similarity Search Explained](https://www.pinecone.io/learn/vector-similarity/)\n",
    "- [The Illustrated BERT](https://jalammar.github.io/illustrated-bert/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
